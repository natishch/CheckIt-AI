{
  "tasks": [
    {
      "task_name": "Project Initialization (uv + full repo skeleton)",
      "description": "Initialize the repository using uv, create the complete folder/file structure defined in the Technical Design Document, and add baseline configs (ruff, mypy, pytest, .env.example). Ensure the project installs and a smoke test can run.",
      "developer_guide_prompt": "You are a Senior Python Engineer. Create a production-grade repository skeleton for the project 'agentic-historian' using uv as the package/project manager. Add a src-layout package named `agentic_historian`. Include: Streamlit app entry, LangGraph graph module placeholders, Pydantic schemas module, Google search tool placeholder, training script placeholders, tests folder with a trivial smoke test, CI workflow skeleton, docs folder with placeholder markdown files, and scripts folder with run/train commands. Add ruff configuration, mypy config, pytest config, and .env.example. Keep code minimal but correctly importable and runnable. Use type hints everywhere and clean module boundaries. Provide any required pyproject.toml sections for tooling (ruff, mypy, pytest).",
      "claude_init_prompt": "Initialize a new repository for 'agentic-historian' using uv and create the full folder structure.\n\n1) Run:\n- uv init agentic-historian\n- cd agentic-historian\n\n2) Create these directories:\n- docs\n- data/raw\n- data/processed\n- models/base\n- models/lora_adapters\n- notebooks\n- scripts\n- src/agentic_historian/app\n- src/agentic_historian/graph/nodes\n- src/agentic_historian/tools\n- src/agentic_historian/llm\n- src/agentic_historian/training\n- src/agentic_historian/utils\n- src/agentic_historian/types\n- tests\n- .github/workflows\n\n3) Create these files with minimal but valid content:\n- src/agentic_historian/__init__.py\n- src/agentic_historian/config.py\n- src/agentic_historian/app/streamlit_app.py\n- src/agentic_historian/graph/state.py\n- src/agentic_historian/graph/graph.py\n- src/agentic_historian/graph/nodes/router.py\n- src/agentic_historian/graph/nodes/researcher.py\n- src/agentic_historian/graph/nodes/fact_analyst.py\n- src/agentic_historian/graph/nodes/writer.py\n- src/agentic_historian/tools/google_search.py\n- src/agentic_historian/llm/prompts.py\n- src/agentic_historian/llm/loader.py\n- src/agentic_historian/llm/generation.py\n- src/agentic_historian/training/data_prep.py\n- src/agentic_historian/training/train_lora.py\n- src/agentic_historian/training/eval_truthfulqa.py\n- src/agentic_historian/utils/logging.py\n- src/agentic_historian/utils/cache.py\n- src/agentic_historian/utils/text.py\n- src/agentic_historian/types/schemas.py\n- docs/technical_design.md\n- docs/evaluation_plan.md\n- docs/demo_script.md\n- data/README.md\n- scripts/run_app.sh\n- scripts/train_lora.sh\n- scripts/eval.sh\n- tests/test_graph_smoke.py\n- .env.example\n- ruff.toml\n- mypy.ini\n- pytest.ini\n- .github/workflows/ci.yml\n\n4) Add dependencies using uv:\n- uv add streamlit pydantic httpx python-dotenv\n- uv add langgraph langchain\n- uv add torch transformers datasets peft accelerate\n- uv add --dev ruff mypy pytest\n\n5) Ensure `uv sync` succeeds and `uv run pytest -q` passes with the smoke test."
    },
    {
      "task_name": "Core Pydantic Schemas (Search, Evidence, Answer)",
      "description": "Implement strict Pydantic models for SearchQuery, SearchResult, EvidenceItem, EvidenceBundle, Finding, Citation, and the LangGraph shared state model. Add serialization helpers and validation rules (e.g., max results, required URLs, evidence ID formatting).",
      "developer_guide_prompt": "Implement Pydantic v2 models for the Agentic Historian. Create `src/agentic_historian/types/schemas.py` and `src/agentic_historian/graph/state.py`. Define: SearchQuery, SearchResult (title, snippet, url, display_domain, rank), EvidenceItem (id like E1, source fields, snippet, url), Finding (claim text, verdict enum, evidence_ids), EvidenceBundle (items, findings, overall_verdict enum), Citation (evidence_id, url), and AgentState (user_query, route, search_queries, search_results, evidence_bundle, final_answer, citations, confidence, run_metadata). Add validators to enforce: evidence_id pattern ^E\\d+$, urls are valid http(s), max search results default 10, and confidence in [0,1]. Provide `.model_dump()` friendly shapes for Streamlit rendering.",
      "claude_init_prompt": "Create/overwrite:\n- src/agentic_historian/types/schemas.py\n- src/agentic_historian/graph/state.py\n\nImplement Pydantic v2 models as described. Ensure imports are correct and the package exports the key types. Add a small unit test in `tests/test_schemas_validation.py` that checks evidence_id validation and confidence range constraints."
    },
    {
      "task_name": "Google Custom Search Tool Client (No-Scrape)",
      "description": "Build a robust Google Custom Search JSON API client using httpx. Support query params, API key/CX from env, retries/backoff, and response normalization into SearchResult objects. Include basic caching hooks.",
      "developer_guide_prompt": "Implement `src/agentic_historian/tools/google_search.py` as a no-scrape retrieval tool using Google Custom Search JSON API. Read env vars: GOOGLE_API_KEY and GOOGLE_CSE_ID (cx). Implement a `GoogleSearchClient` with `search(query: str, num_results: int = 10) -> list[SearchResult]`. Use httpx with timeouts, retry/backoff for transient failures, and raise typed exceptions on quota/auth errors. Normalize results to the Pydantic SearchResult schema. Do not fetch page contents—only use the API response fields. Add a small test with mocked httpx transport verifying parsing and error handling.",
      "claude_init_prompt": "Create/overwrite `src/agentic_historian/tools/google_search.py` with:\n- GoogleSearchClient using httpx\n- typed exceptions (AuthError, QuotaError, NetworkError)\n- env loading via `agentic_historian/config.py`\n\nAdd tests:\n- `tests/test_google_tool.py` mocking httpx to return a sample Custom Search JSON payload and verifying it maps correctly to SearchResult.\n\nUpdate `.env.example` to include GOOGLE_API_KEY and GOOGLE_CSE_ID placeholders."
    },
    {
      "task_name": "LangGraph Pipeline Assembly (Router → Researcher → Analyst → Writer)",
      "description": "Implement the LangGraph graph with typed state, nodes, and edges. Router selects the route, then executes the full pipeline for fact-check requests. Include a CLI entry/helper for running the graph in isolation.",
      "developer_guide_prompt": "Implement `src/agentic_historian/graph/graph.py` to build the LangGraph pipeline. Use a Pydantic AgentState. Add nodes: router_node, researcher_node, fact_analyst_node, writer_node. Router sets state.route and either short-circuits (clarify/out_of_scope) or proceeds. Researcher calls GoogleSearchClient and populates state.search_results. Fact Analyst constructs EvidenceBundle. Writer produces final_answer + citations + confidence. Ensure the graph is importable and a `run_graph(user_query: str)` function exists for Streamlit to call. Add a smoke test that runs the graph with a mocked researcher node returning deterministic results.",
      "claude_init_prompt": "Create/overwrite:\n- src/agentic_historian/graph/graph.py\n- src/agentic_historian/graph/nodes/router.py\n- src/agentic_historian/graph/nodes/researcher.py\n- src/agentic_historian/graph/nodes/fact_analyst.py\n- src/agentic_historian/graph/nodes/writer.py\n\nImplement minimal but working nodes and graph assembly. Add `tests/test_graph_smoke.py` that monkeypatches the researcher to avoid real API calls and asserts the graph returns a final_answer and citations."
    },
    {
      "task_name": "Router Node (Intent + Clarification Logic)",
      "description": "Implement a deterministic router that classifies queries into fact_check, clarify, or out_of_scope using rules + optional lightweight model hook. Provide explainable routing metadata.",
      "developer_guide_prompt": "Implement router logic in `src/agentic_historian/graph/nodes/router.py`. Use deterministic heuristics: detect empty/underspecified queries (clarify), detect non-historical creative requests (out_of_scope), else fact_check. Store route decision and reasons into state.run_metadata['router'] for UI display. Write unit tests for key examples.",
      "claude_init_prompt": "Update `src/agentic_historian/graph/nodes/router.py` to include deterministic routing plus metadata logging, and create `tests/test_router.py` with 8-10 representative queries verifying routing behavior."
    },
    {
      "task_name": "Researcher Node (Query Expansion + Retrieval)",
      "description": "Generate 1–3 high-quality search queries from the user prompt, call the Google API client, and normalize results. Add caching and domain-restriction option.",
      "developer_guide_prompt": "Implement `src/agentic_historian/graph/nodes/researcher.py`. Build a query-expansion function that creates up to 3 search queries from the user query (handle dates, names, disambiguation). Call GoogleSearchClient and merge results, keeping best-ranked unique URLs. Add an optional trusted-domain mode (config flag) that applies site: filters for authoritative sources when possible. Update state.search_queries and state.search_results. Add unit tests with mocked GoogleSearchClient.",
      "claude_init_prompt": "Update `src/agentic_historian/graph/nodes/researcher.py` to include query expansion, deduping, and optional `site:` restriction. Add `tests/test_researcher.py` using a fake client returning deterministic results."
    },
    {
      "task_name": "Fact Analyst Node (Evidence Bundle + Contradictions)",
      "description": "Score sources, extract candidate claims from snippets, detect contradictions, and produce a compact EvidenceBundle with numbered evidence IDs and a verdict label.",
      "developer_guide_prompt": "Implement `src/agentic_historian/graph/nodes/fact_analyst.py`. Input: state.search_results. Output: EvidenceBundle with EvidenceItems E1..EN, Findings (claim text, verdict enum, evidence IDs), and an overall verdict (supported/not_supported/contested/insufficient). Use heuristic source scoring (domain signals, consistency). Implement contradiction detection by identifying conflicting dates/numbers across snippets for the same named entity. Keep it lightweight and explainable (store scoring details into run_metadata). Add tests that feed synthetic SearchResults and verify contested vs supported outcomes.",
      "claude_init_prompt": "Create/overwrite `src/agentic_historian/graph/nodes/fact_analyst.py` with evidence bundling, scoring, and contradiction labeling. Add `tests/test_fact_analyst.py` with 3 scenarios: supported, contested, insufficient."
    },
    {
      "task_name": "Writer Node (Fine-Tuned LoRA Model Inference + Citation Enforcement)",
      "description": "Load the base model + LoRA adapter, generate evidence-grounded answers, and enforce citation usage and uncertainty handling. Add a post-generation validator that rejects uncited claims.",
      "developer_guide_prompt": "Implement `src/agentic_historian/graph/nodes/writer.py` plus `src/agentic_historian/llm/loader.py` and `src/agentic_historian/llm/prompts.py`. Load a Hugging Face causal LM and attach a LoRA adapter via PEFT. Provide a `generate_answer(evidence_bundle, user_query)` function that returns (final_answer, citations, confidence). The prompt must: (1) forbid using knowledge outside evidence, (2) require evidence IDs like [E1], (3) require an explicit 'limitations' note when contested/insufficient. Add a validator: if answer contains no evidence IDs, force a safe fallback response. Add tests for formatting and citation presence.",
      "claude_init_prompt": "Update:\n- src/agentic_historian/llm/loader.py (HF model + PEFT adapter loader)\n- src/agentic_historian/llm/prompts.py (strict templates)\n- src/agentic_historian/graph/nodes/writer.py (node wiring + validator)\n\nAdd `tests/test_writer_constraints.py` verifying that generated output includes evidence IDs or falls back safely when missing."
    },
    {
      "task_name": "Training Pipeline (TruthfulQA → LoRA SFT)",
      "description": "Implement dataset preparation and LoRA fine-tuning scripts using Hugging Face Datasets + Transformers + PEFT. Save adapters and training logs, and ensure reproducibility via config.",
      "developer_guide_prompt": "Implement `src/agentic_historian/training/data_prep.py` and `src/agentic_historian/training/train_lora.py` for LoRA SFT on a TruthfulQA-style dataset. Use HF datasets to load truthful_qa (or a prepared local export), format into instruction-response pairs emphasizing truthful/refusal behavior, tokenize, and train with Transformers Trainer or Accelerate. Use PEFT LoRA config targeting attention projection modules. Save adapter weights to `models/lora_adapters/<run_name>/`. Log metrics and config to a JSON file. Provide a script entry that can be run with uv. Keep defaults lightweight (small batch, gradient accumulation).",
      "claude_init_prompt": "Create/overwrite:\n- src/agentic_historian/training/data_prep.py\n- src/agentic_historian/training/train_lora.py\n- scripts/train_lora.sh\n\nImplement a runnable training script with CLI args (base_model, output_dir, epochs, lr, max_steps, batch_size). Ensure it saves PEFT adapter weights. Add a short README section in README.md describing how to run training."
    },
    {
      "task_name": "Evaluation Harness (Truthfulness + Refusal + E2E)",
      "description": "Implement offline evaluation for TruthfulQA-like prompts and an end-to-end evaluation script that runs the full graph on a curated query set, outputting JSON results for analysis.",
      "developer_guide_prompt": "Implement `src/agentic_historian/training/eval_truthfulqa.py` and a simple E2E evaluator that runs the LangGraph pipeline on a small curated set of historical queries stored in `docs/evaluation_plan.md` or `data/processed/e2e_eval.jsonl`. Output per-sample JSON containing: query, verdict, answer, citations, confidence, and run_metadata. Provide aggregate summary stats (refusal rate on unanswerables, citation rate, contested rate).",
      "claude_init_prompt": "Create/overwrite:\n- src/agentic_historian/training/eval_truthfulqa.py\n- scripts/eval.sh\n\nAdd a small `data/processed/e2e_eval.jsonl` with 10 sample queries. Ensure the evaluator writes `artifacts/eval_results.jsonl` and prints summary stats."
    },
    {
      "task_name": "Streamlit UI (Chat + Evidence Panel + Run Metadata)",
      "description": "Build the Streamlit UI with a chat interface, evidence viewer (E1..En), and expandable run metadata. Support streaming tokens if feasible and provide a demo-friendly layout.",
      "developer_guide_prompt": "Implement `src/agentic_historian/app/streamlit_app.py`. Provide: (1) chat input, (2) message history, (3) a right-side panel showing Evidence Items with clickable URLs, (4) verdict badge (supported/contested/insufficient), (5) confidence display, and (6) expandable run metadata (router decision, timing, cache hits). Call `run_graph(user_query)` from the graph module. Add local state management and an option to disable live search for offline demo mode (use a fixture evidence bundle).",
      "claude_init_prompt": "Create/overwrite `src/agentic_historian/app/streamlit_app.py` with a polished Streamlit chat UI and evidence panel. Add `scripts/run_app.sh` that runs: `uv run streamlit run src/agentic_historian/app/streamlit_app.py`."
    },
    {
      "task_name": "Caching + Logging + Config (Demo Reliability)",
      "description": "Add structured logging, simple disk caching for searches, and centralized config management for env vars and feature flags.",
      "developer_guide_prompt": "Implement `src/agentic_historian/config.py`, `src/agentic_historian/utils/cache.py`, and `src/agentic_historian/utils/logging.py`. Use environment variables for API keys and runtime flags (OFFLINE_MODE, TRUSTED_DOMAINS_ONLY, MAX_RESULTS). Implement a disk cache (JSON) keyed by query and day. Add structured logs with correlation IDs per run and include node timing measurements stored into state.run_metadata.",
      "claude_init_prompt": "Update:\n- src/agentic_historian/config.py\n- src/agentic_historian/utils/cache.py\n- src/agentic_historian/utils/logging.py\n\nAdd cache integration to the researcher node and timing metadata around each node execution. Ensure `.env.example` lists all env vars used."
    },
    {
      "task_name": "Testing + CI (Quality Gate for Submission)",
      "description": "Add unit tests for nodes and schemas, plus a CI workflow that runs ruff, mypy (best-effort), and pytest on push.",
      "developer_guide_prompt": "Create a GitHub Actions workflow that installs uv, runs `uv sync`, then runs `uv run ruff check .`, `uv run mypy src` (allow failures if needed but prefer strict), and `uv run pytest -q`. Ensure tests do not call external APIs; mock all network access. Add a minimal set of tests: router, schemas, researcher parsing, fact analyst verdicting, graph smoke.",
      "claude_init_prompt": "Create/overwrite `.github/workflows/ci.yml` to run uv-based lint/type/test checks on ubuntu-latest. Ensure the workflow caches uv downloads if possible. Confirm tests run without external API calls."
    }
  ]
}
